{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a65989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import PIL \n",
    "from PIL import Image \n",
    "import sklearn \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import cv2\n",
    "import fcswrite\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006dae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single FOV, all in one timepoint/folder (Standard export format)\n",
    "\n",
    "\n",
    "\n",
    "def process_grayscale_images_by_well(folder_path):\n",
    "    expected_channels = [\"w1\", \"w2\", \"w3\", \"w4\"]\n",
    "    well_pattern = re.compile(r'([A-F][0-9]{2})')\n",
    "    channel_pattern = re.compile(r'(w[1-4])')\n",
    "\n",
    "    images_by_well = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.startswith(\"._\") or not filename.endswith(\".TIF\"):\n",
    "            continue\n",
    "\n",
    "        well_match = well_pattern.search(filename)\n",
    "        channel_match = channel_pattern.search(filename)\n",
    "\n",
    "        if well_match and channel_match:\n",
    "            well = well_match.group(1)\n",
    "            channel = channel_match.group(1)\n",
    "\n",
    "            images_by_well.setdefault(well, {})[channel] = os.path.join(folder_path, filename)\n",
    "\n",
    "    for well, channel_files in images_by_well.items():\n",
    "        print(f\"Processing well: {well}\")\n",
    "        data = {\"X\": [], \"Y\": []}\n",
    "        all_positions = []\n",
    "        \n",
    "        for channel in expected_channels:\n",
    "            if channel in channel_files:\n",
    "                image = Image.open(channel_files[channel])\n",
    "                image_array = np.array(image)\n",
    "\n",
    "                # #BACKGROUND SUBTRACTION\n",
    "                # percentile_subtract = np.percentile(image_array, 50)\n",
    "                # image_array = np.clip(image_array - percentile_subtract, a_min=0, a_max=None)\n",
    "                # #BACKGROUND SUBTRACTION\n",
    "\n",
    "                height, width = image_array.shape\n",
    "\n",
    "                if len(all_positions) == 0:\n",
    "                    positions = np.indices((height, width)).reshape(2, -1).T\n",
    "                    all_positions = positions\n",
    "                    data[\"X\"], data[\"Y\"] = positions.T\n",
    "\n",
    "                data[channel] = image_array.flatten()\n",
    "            else:\n",
    "                data[channel] = np.zeros(len(all_positions))\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"total_intensity\"] = df[expected_channels].sum(axis=1)\n",
    "        df[\"r_y\"] = df['w1']/df['w2']\n",
    "        df[\"r_g\"] = df['w1']/df['w3']\n",
    "        df[\"r_b\"] = df['w1']/df['w4']\n",
    "        df[\"y_g\"] = df['w2']/df['w3']\n",
    "        df[\"y_b\"] = df['w2']/df['w4']\n",
    "        df[\"g_b\"] = df['w3']/df['w4']\n",
    "\n",
    "\n",
    "\n",
    "        for channel in expected_channels:\n",
    "            df[f\"p_{channel}\"] = np.where(df[\"total_intensity\"] > 0,\n",
    "                                          df[channel] / df[\"total_intensity\"], 0)\n",
    "\n",
    "        #Change to fit your nomenclature \n",
    "        output_fcs_path = os.path.join(folder_path, f\"{well}.fcs\")\n",
    "        parameter_names = [\"X\", \"Y\", \"texas_red\", \"yfp\", \"fitc\", \"cfp\", \"total_intensity\", \"r_y\", \"r_g\", \"r_b\", \"y_g\", \"y_b\", \"g_b\"]\n",
    "        data_matrix = df[[\"X\", \"Y\", \"w1\", \"w2\", \"w3\", \"w4\", \"total_intensity\", \"r_y\", \"r_g\", \"r_b\", \"y_g\", \"y_b\", \"g_b\"]].to_numpy(dtype=np.float64)\n",
    "        fcswrite.write_fcs(output_fcs_path, parameter_names, data_matrix)\n",
    "\n",
    "        #Use below to export to CSV to troubleshoot\n",
    "        # df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        print(f\"Saved FCS for well {well} to {output_fcs_path}\")\n",
    "\n",
    "# Run on the full image folder, change \n",
    "image_folder = \"/Volumes/Elements/8sh Chemotherapy/8sh panel1 s1 p1d6 200ms CFP_Plate_54570/TimePoint_1\"\n",
    "\n",
    "process_grayscale_images_by_well(image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59690f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder of folders\n",
    "\n",
    "from fcswrite import write_fcs\n",
    "\n",
    "def process_all_plates(root_folder):\n",
    "    expected_channels = [\"w1\", \"w2\", 'w3', 'w4']\n",
    "    well_pattern = re.compile(r'([A-F][0-9]{2})')\n",
    "    channel_pattern = re.compile(r'(w[1-4])')\n",
    "\n",
    "    output_base = \"/Volumes/Elements/Dox NH2 July 2025/Per Well Median Background Subtraction\"\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "    for subfolder in os.listdir(root_folder):\n",
    "        subfolder_path = os.path.join(root_folder, subfolder)\n",
    "        timepoint_path = os.path.join(subfolder_path, \"TimePoint_1\")\n",
    "\n",
    "        if not os.path.isdir(timepoint_path):\n",
    "            continue\n",
    "\n",
    "        # Extract prefix for file naming (e.g., d0p1)\n",
    "        prefix_match = re.search(r'D(\\d+)\\s+P(\\d+)', subfolder, re.IGNORECASE)\n",
    "        if not prefix_match:\n",
    "            print(f\"Skipping {subfolder}, couldn't parse day/plate info.\")\n",
    "            continue\n",
    "        prefix = f\"d{prefix_match.group(1).lower()}p{prefix_match.group(2).lower()}\"\n",
    "\n",
    "        images_by_well = {}\n",
    "        for filename in os.listdir(timepoint_path):\n",
    "            if filename.startswith(\"._\") or not filename.endswith(\".TIF\"):\n",
    "                continue\n",
    "\n",
    "            well_match = well_pattern.search(filename)\n",
    "            channel_match = channel_pattern.search(filename)\n",
    "\n",
    "            if well_match and channel_match:\n",
    "                well = well_match.group(1)\n",
    "                channel = channel_match.group(1)\n",
    "                images_by_well.setdefault(well, {})[channel] = os.path.join(timepoint_path, filename)\n",
    "\n",
    "        for well, channel_files in images_by_well.items():\n",
    "            print(f\"Processing well: {well} from {subfolder}\")\n",
    "            data = {\"X\": [], \"Y\": []}\n",
    "            all_positions = []\n",
    "\n",
    "            for channel in expected_channels:\n",
    "                if channel in channel_files:\n",
    "                    image = Image.open(channel_files[channel])\n",
    "                    image_array = np.array(image)\n",
    "\n",
    "                    #Background Subtraction\n",
    "                    percentile_subtract = np.percentile(image_array, 50)\n",
    "                    image_array = np.clip(image_array - percentile_subtract, a_min=0, a_max=None)\n",
    "                    #Background Subtraction\n",
    "            \n",
    "                    height, width = image_array.shape\n",
    "\n",
    "                    if len(all_positions) == 0:\n",
    "                        positions = np.indices((height, width)).reshape(2, -1).T\n",
    "                        all_positions = positions\n",
    "                        data[\"X\"], data[\"Y\"] = positions.T\n",
    "\n",
    "                    data[channel] = image_array.flatten()\n",
    "                else:\n",
    "                    data[channel] = np.zeros(len(all_positions))\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"total_intensity\"] = df[expected_channels].sum(axis=1)\n",
    "            df[\"r_y\"] = df['w1']/df['w2']\n",
    "            df[\"r_g\"] = df['w1']/df['w3']\n",
    "            df[\"r_b\"] = df['w1']/df['w4']\n",
    "            df[\"y_g\"] = df['w2']/df['w3']\n",
    "            df[\"y_b\"] = df['w2']/df['w4']\n",
    "            df[\"g_b\"] = df['w3']/df['w4']\n",
    "            \n",
    "\n",
    "            for channel in expected_channels:\n",
    "                df[f\"p_{channel}\"] = np.where(df[\"total_intensity\"] > 0,\n",
    "                                              df[channel] / df[\"total_intensity\"], 0)\n",
    "\n",
    "\n",
    "            parameter_names = [\"X\", \"Y\", \"texas_red\", \"yfp\", \"fitc\", \"cfp\", \"total_intensity\", \"r_y\", \"r_g\", \"r_b\", \"y_g\", \"y_b\", \"g_b\"]\n",
    "            data_matrix = df[[\"X\", \"Y\", \"w1\", \"w2\", \"w3\", \"w4\", \"total_intensity\", \"r_y\", \"r_g\", \"r_b\", \"y_g\", \"y_b\", \"g_b\"]].to_numpy(dtype=np.float64)\n",
    "\n",
    "            output_fcs_path = os.path.join(output_base, f\"{prefix}_{well}.fcs\")\n",
    "            write_fcs(output_fcs_path, parameter_names, data_matrix)\n",
    "            print(f\"Saved: {output_fcs_path}\")\n",
    "\n",
    "# Usage\n",
    "root_folder = \"/Volumes/Elements/Dox NH2 July 2025/Raw Data\"\n",
    "process_all_plates(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsample\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fcsparser import parse\n",
    "from fcswrite import write_fcs\n",
    "\n",
    "def merge_downsample_fcs(folder_path, pct_kept=0.5, seed_base=42):\n",
    "  \n",
    "    fcs_files = sorted(p for p in Path(folder_path).iterdir() if p.suffix.lower() == \".fcs\")\n",
    "    if not fcs_files:\n",
    "        print(f\"No .fcs files found in {folder_path}\")\n",
    "        return\n",
    "\n",
    "    samples = []\n",
    "    col_sets = []\n",
    "\n",
    "    for i, fpath in enumerate(fcs_files, start=1):\n",
    "        try:\n",
    "            meta, data = parse(str(fpath), reformat_meta=True)\n",
    "            data.columns = [str(c) for c in data.columns]\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        n = len(data)\n",
    "        k = max(1, int(round(n * (pct_kept / 100.0))))\n",
    "        if k < n:\n",
    "            rng = np.random.default_rng(seed_base + i)\n",
    "            idx = rng.choice(n, size=k, replace=False)\n",
    "            data = data.iloc[idx]\n",
    "\n",
    "        samples.append(data)\n",
    "        col_sets.append(set(data.columns))\n",
    "        print(f\"  - {fpath.name}: {n:,} → {len(data):,} events\")\n",
    "\n",
    "    if not samples:\n",
    "        print(\"No data collected.\")\n",
    "        return\n",
    "\n",
    "    common_cols = sorted(set.intersection(*col_sets))\n",
    "    if not common_cols:\n",
    "        print(\"No common channels across files.\")\n",
    "        return\n",
    "    if any(set(df.columns) != set(common_cols) for df in samples):\n",
    "        print(f\"[INFO] Restricting to {len(common_cols)} common channels\")\n",
    "        print(\"       \" + \", \".join(common_cols))\n",
    "\n",
    "    merged = pd.concat([df[common_cols] for df in samples], axis=0, ignore_index=True)\n",
    "    print(f\"Merged total events: {len(merged):,}\")\n",
    "\n",
    "    # Clean numeric data\n",
    "    for c in common_cols:\n",
    "        if not np.issubdtype(merged[c].dtype, np.number):\n",
    "            merged[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n",
    "    merged = merged.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    output_path = Path(folder_path) / \"merged_downsampled.fcs\"\n",
    "    write_fcs(str(output_path), chn_names=common_cols, data=merged.to_numpy(np.float32))\n",
    "    print(f\"Saved merged FCS to {output_path}\")\n",
    "\n",
    "\n",
    "fcs_folder = \"/Volumes/Elements/MM Exp 25 31/FCS Files/First Image\"\n",
    "merge_downsample_fcs(fcs_folder, pct_kept=0.5, seed_base=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
